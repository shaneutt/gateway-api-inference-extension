# ------------------------------------------------------------------------------
# vLLM Deployment
#
# This deploys the full vLLM model server, capable of serving real models such
# as Llama 3.1-8B-Instruct via the OpenAI-compatible API. It is intended for
# environments with GPU resources and where full inference capabilities are
# required.
#
# The deployment can be customized using environment variables to set:
#   - The container image and tag (VLLM_IMAGE, VLLM_TAG)
#   - The model to load (MODEL_NAME)
#
# This setup is suitable for testing on Kubernetes (including
# GPU-enabled nodes or clusters with scheduling for `nvidia.com/gpu`).
# -----------------------------------------------------------------------------
kind: Kustomization

resources:
- deployments.yaml
- secret.yaml
- configmap.yaml


images:
- name: vllm/vllm-openai
  newName: ${VLLM_IMAGE}
  newTag: ${VLLM_TAG}

- name: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/lora-syncer
  newName: ${LORA_ADAPTER_SYNCER_IMAGE}
  newTag: ${LORA_ADAPTER_SYNCER_TAG}

configMapGenerator:
- name: vllm-model-config
  literals:
    - MODEL_NAME=${MODEL_NAME}
